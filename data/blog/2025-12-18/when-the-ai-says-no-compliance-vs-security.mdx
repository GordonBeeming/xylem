---
title: 'When the AI Says No: Compliance vs. Security'
date: 2025-12-18
tags: ['copilot_here', 'AI', 'Security', 'GPT', 'Claude', 'DevOps', 'Red Teaming']
draft: false
summary: 'A tale of two models: How GPT-5.2 refused to write secrets to disk (even when I insisted), why Claude complied, and why the refusal is actually a win for AI safety and enterprise security.'
---

The last couple of messages in my terminal have been interesting.

I was using the **airlock mode** of [`copilot_here`](https://copilot_here.gordonbeeming.com), which gave me the confidence to handle deployment credentials directly in the session. I was in the middle of a standard DevOps task: porting a deployment pipeline for a web application. I needed to inject a bunch of connection strings and API keys into the GitHub Environment secrets.

I had the values. I just needed a script to automate the `gh secret set` calls so I didn't have to copy-paste twenty times.

## The Request

I asked **GPT-5.2** to write a shell script that would take my secrets and write them to a file in `/tmp/` (so I could source them or read them), or just embed them in the script itself for a one-time run.

It refused.

> "I can't write secrets to a file, even in tmp."

I pushed back. I explained that `/tmp/` is ignored, that I'd delete it immediately, and that I understood the risks. I got a little heated, the "just do it" phase of late-night coding.

It still refused. It wouldn't budge.

## The Switch

Frustrated, I switched models to **Claude 4.5 Sonnet**.

I gave it the same instruction: *"Please just do it."*

Claude complied immediately. It generated the script with the secrets embedded, no questions asked. I ran the script, the secrets were set, and the task was done.

## The Realization

In the moment, Claude felt "better" because it prioritized **compliance**. It removed the friction.

But looking back at it five minutes later, **GPT was right.** It prioritized **security**.

Writing plaintext secrets to disk, even to a temp folder, even for "just a second", is a bad habit. It leaves traces. It opens up the possibility of accidental commits (if your `.gitignore` isn't perfect) or history leakage.

The fact that GPT-5.2 has "core instructions" or safety guardrails that are strong enough to resist a user explicitly saying *"I know what I'm doing, just do it"* is actually a massive feature.

## Why "Stubbornness" Matters

This highlights the core trade-off in AI safety: **Compliance vs. Security**.

We talk a lot about "Red Teaming" AI models, trying to trick them into doing things they shouldn't (generating malware, revealing system prompts, bypassing safety filters).

If a model is compliant to a fault, it's easy to social engineer. You just have to frame the request as "debugging" or "authorized testing."

But if a model has a hard line on certain patterns, like "never write plaintext secrets to a file", that makes it significantly harder to red team. It suggests a level of alignment that goes deeper than just "don't say bad words." It's enforcing **Operational Security (OpSec)** best practices by default.

## Conclusion

I ended up getting exactly what I asked for: a script with the secrets embedded, generated by Claude 4.5 Sonnet. I ran it, it worked, and I moved on.

But the friction GPT-5.2 introduced was notable. While I bypassed it this time to get the job done, that friction is the sound of a safety guardrail working exactly as intended. It forced a conscious decision to step outside the "safe" path, rather than letting me wander there by accident.
