---
title: 'The Conprism: The Illusion of Precision in Estimates and Star Ratings'
date: '2025-09-15'
tags: ['Productivity', 'Estimation', 'Feedback', 'Ratings', 'Cognitive Bias']
draft: false
summary: 'It started with a typo and a thought about 10-point rating scales. I realized our quest for precision in work estimates and user ratings often creates a "conprism", a false sense of accuracy. Here''s why less is usually more.'
---

ğŸ¤ A Human-AI Collaboration - I worked with my AI assistant to bring this post to life. Think of it as a brainstorming partner that helps structure thoughts and smooth out the prose. The core message and the experience behind it? That's all me.

I was thinking recently about rating scales. When you're asked to give a rating on a 10-point scale, what's the *real* difference between a 7 and an 8? It feels a bit like splitting hairs, and it seems like the actual feeling can get lost in the search for an arbitrary number.

It got me thinking about how often we encounter this same pattern in our professional lives. We like data and precision, believing that more granularity, more options, leads to more accuracy.

But what if, beyond a certain point, more options actually lead to *less* meaningful data? What if our quest for precision is actually deceiving us?

### The Birth of a Word: Conprism

So, what's with the word "conprism" in the title? Full disclosure: it was a complete typo! ğŸ˜… I was trying to type "comparison" while brainstorming this very idea, and my fingers had other plans. But the more I looked at the word, the more perfect it felt.

A **conprism** is a lens that gives us a false sense of precision, making us think we're being more accurate than we really are. It's like looking through a pane of glass that's not perfectly flat, the view is clear enough, but the details are slightly off.

This conprism pops up, especially in work estimates and user ratings.

## Through the Conprism of Work Estimates

In the world of software development, the pressure for precise estimates is pretty common. "How many hours will Feature X take?" The desire for predictable outcomes leads to a demand for exact numbers.

But when we ask for hour-by-hour estimates for complex, uncertain tasks, we're looking right through the conprism. Think about it:

* **1 hour vs. 2 hours?** You can probably feel the difference.
* **5 hours vs. 8 hours?** A bit harder, but still tangible.
* **80 hours vs. 120 hours?** This is educated guesswork, at best. The difference often feels arbitrary.

The challenge is that these highly granular, arbitrary numbers are then treated as commitments. They become the "source of truth," even when that truth is on shaky ground. This can lead to stress, "sandbagging" (padding estimates), and stakeholders having a false confidence that doesn't match reality.

### Breaking Free: Embracing Simplicity

The solution is to step away from the conprism and use tools that embrace uncertainty. Many agile teams have already figured this out:

1.  **T-Shirt Sizing:** Estimate in sizes like Small, Medium, or Large. It communicates magnitude without the pretense of precision. A "Large" task is clearly bigger than a "Small" one, and that's often all you need to know to make a good decision.

<Figure key="/images/conprism-tshirt-sizing.png" src="/images/conprism-tshirt-sizing.png" alt="An illustration showing T-shirt sizes (S, M, L, XL) with corresponding rough effort levels. The image conveys scaling effort without exact precision." width="0" height="0" caption="T-Shirt Sizing: Communicating effort, not false precision." />

2.  **Fibonacci Sequencing:** Using points like 1, 2, 3, 5, 8, 13... is another great tool. The increasing gaps between numbers inherently acknowledge that uncertainty grows with size. The jump from 13 to 21 forces a conversation about the significant unknowns, which is far more valuable than pretending you know the difference between 18 and 19.

<Figure key="/images/conprism-fibonacci-poker.png" src="/images/conprism-fibonacci-poker.png" alt="An image showing planning poker cards with Fibonacci numbers (1, 2, 3, 5, 8, 13, 21). The cards are fanned out to show the increasing values and gaps." width="0" height="0" caption="Fibonacci numbers in planning poker acknowledge growing uncertainty." />

These methods foster honest conversations about uncertainty, rather than forcing a deceptive precision.

## Through the Conprism of Ratings

This brings me back to the initial thought about rating scales. The conprism doesn't just distort project plans; it messes with how we collect feedback.

While mulling over 10-point scales, I realized my preference. A simple "like vs. dislike" can feel too restrictive. But a 10-point scale can be a bit overwhelming.

### The "Goldilocks" Scale: Why 5 is Just Right

For me, especially for rating things like a service or a product experience, a **5-point scale** is the "Goldilocks" sweet spot: not too simple, not too complex, but *just right*.

Here's why I think it works so well:

**It's Brain-Friendly:** Research often shows that humans can reliably distinguish between 5 to 7 levels of something. More options just introduce noise.

**Clear, Meaningful Nuance:** It provides a clear spectrum without the hair-splitting.

â­ï¸ (1): Terrible.  
â­ï¸â­ï¸ (2): Not great.  
â­ï¸â­ï¸â­ï¸ (3): It was fine. (This neutral option is so important!)  
â­ï¸â­ï¸â­ï¸â­ï¸ (4): Good.  
â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ (5): Loved it!

**The Power of "Meh":** That neutral 3-star option is crucial. It's an honest response for people who don't feel strongly either way. Forcing them to pick a positive or negative side can pollute your data. Knowing that many of your users are just "neutral" is valuable information!

When we use 10-point scales, we're back in conprism territory. The distinctions start to feel fuzzy and create unnecessary guesswork. Just trying to define them highlights the problem:

â­ï¸ (1): Terrible.  
â­ï¸â­ï¸ (2): Really not good.  
â­ï¸â­ï¸â­ï¸ (3): Pretty bad.  
â­ï¸â­ï¸â­ï¸â­ï¸ (4): More bad than good.  
â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ (5): Perfectly average, leaning towards disappointment. (Negative Meh)  
â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ (6): Perfectly average, leaning towards pleasant. (Positive Meh)  
â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ (7): Good.  
â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ (8): A bit better than good.  
â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ (9): Genuinely great.  
â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ (10): Excellent, perfect.

See the problem? What does "Negative Meh" vs. "Positive Meh" even mean in practice? The distinction between a 5 and a 6, or a 7 ("Good") and an 8 ("A bit better than good"), is almost entirely subjective. It forces people to invent their own meaning, which creates decision fatigue and results in data that looks precise on the surface but is built on a foundation of inconsistent, fuzzy feelings.

## A Final Thought: Stepping Away from the Conprism

Before you go, I want to add a quick thought. If you're reading this and feeling a bit called out or like your team's processes are being criticized, please know that's not my intention at all!

This entire post was born from a simple thought I had about the ambiguity of 10-point rating scales, and the surprising parallel I saw with the vagueness of large work estimates in the software development world. It was just a thought connecting two things I'd noticed, and I wanted to share it.

By questioning our need for minute precision and embracing simpler tools, we can gain a much clearer, more honest view of our work and the world. Sometimes, less data, thoughtfully collected, is truly more.